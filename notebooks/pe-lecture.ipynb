{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1UXJS3pHqKo"
      },
      "source": [
        "# Getting Started with Prompt Engineering\n",
        "by Kazhian Muthusami\n",
        "\n",
        "\n",
        "This notebook contains examples and exercises to learning about prompt engineering.\n",
        "\n",
        "We will be using the [OpenAI APIs](https://platform.openai.com/) for all examples. I am using the default settings `temperature=0.7` and `top-p=1`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7-R43ooHqKq"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze3RfzQNHqKq"
      },
      "source": [
        "## 1. Prompt Engineering Basics\n",
        "\n",
        "Objectives\n",
        "- Load the libraries\n",
        "- Review the format\n",
        "- Cover basic prompts\n",
        "- Review common use cases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrRi_O3ZHqKq"
      },
      "source": [
        "Below we are loading the necessary libraries, utilities, and configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aa1EngpnHqKr",
        "outputId": "ccf2cc0f-1691-49bf-d41d-b3530217eea5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "# update or install the necessary libraries\n",
        "!pip install openai==1.55.3 tiktoken==0.6 session-info --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4oBg1Tj2HqKr"
      },
      "outputs": [],
      "source": [
        "# Import all Python packages required to access the Azure Open AI API\n",
        "import json\n",
        "import tiktoken\n",
        "import session_info\n",
        "import IPython\n",
        "\n",
        "from IPython import display\n",
        "from openai import AzureOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VidbWE9ZHqKr"
      },
      "source": [
        "### Create a json file using below format. Name: `openai-config.json`\n",
        "\n",
        "```\n",
        "{\n",
        "    \"AZURE_OPENAI_KEY\": \"#*9jO#**#Swg8h#*zut*#*mvJv#*99AACYeB*#*OGJa3*\",\n",
        "    \"AZURE_OPENAI_ENDPOINT\": \"https://#bcd1&3*45#.openai.azure.com/\",\n",
        "    \"AZURE_OPENAI_APIVERSION\": \"2024-02-15-preview:\",\n",
        "    \"CHATGPT_MODEL\": \"gpt-4o-mini\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "k278TfD-HqKs"
      },
      "outputs": [],
      "source": [
        "# Azure Open AI credentials and the id of the deployed chat model are stored as\n",
        "# key value pairs in a json file\n",
        "\n",
        "with open('openai-config.json', 'r') as az_creds:\n",
        "    data = az_creds.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-mOS9CSRHqKs"
      },
      "outputs": [],
      "source": [
        "creds = json.loads(data)\n",
        "\n",
        "# Credentials to authenticate to the personalized Open AI model server\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint=creds[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "    api_key=creds[\"AZURE_OPENAI_KEY\"],\n",
        "    api_version=creds[\"AZURE_OPENAI_APIVERSION\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9P6d3QtKHqKs",
        "outputId": "76c9e6c0-7a38-43c4-eeb1-1105c694321d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model I am going to use: gpt-4o-mini-kazhian\n"
          ]
        }
      ],
      "source": [
        "# Deployment namre of the ChatCompletion endpoint\n",
        "CHATGPT_MODEL = creds[\"CHATGPT_MODEL\"]\n",
        "print(f\"The model I am going to use: {creds['CHATGPT_MODEL']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vsHKhnj8HqKt"
      },
      "outputs": [],
      "source": [
        "# Utility methods\n",
        "\n",
        "def set_open_params(\n",
        "    model=creds[\"CHATGPT_MODEL\"],\n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "):\n",
        "    \"\"\" set openai parameters\"\"\"\n",
        "\n",
        "    openai_params = {}\n",
        "\n",
        "    openai_params['model'] = model\n",
        "    openai_params['temperature'] = temperature\n",
        "    openai_params['max_tokens'] = max_tokens\n",
        "    openai_params['top_p'] = top_p\n",
        "    openai_params['frequency_penalty'] = frequency_penalty\n",
        "    openai_params['presence_penalty'] = presence_penalty\n",
        "    return openai_params\n",
        "\n",
        "def get_completion(params, messages):\n",
        "    \"\"\" GET completion from openai api\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model = params['model'],\n",
        "        messages = messages,\n",
        "        temperature = params['temperature'],\n",
        "        max_tokens = params['max_tokens'],\n",
        "        top_p = params['top_p'],\n",
        "        frequency_penalty = params['frequency_penalty'],\n",
        "        presence_penalty = params['presence_penalty'],\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def print_in_markdown(text):\n",
        "    \"\"\" Print text in markdown format\"\"\"\n",
        "    IPython.display.Markdown(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi16lciiHqKt"
      },
      "source": [
        "Basic prompt example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "A73FhT4KHqKt",
        "outputId": "1180c9cd-5b52-4f4d-e76d-4e92c8a9049c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model': 'gpt-4o-mini-kazhian',\n",
              " 'temperature': 0.7,\n",
              " 'max_tokens': 256,\n",
              " 'top_p': 1,\n",
              " 'frequency_penalty': 0,\n",
              " 'presence_penalty': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# basic example\n",
        "params = set_open_params()\n",
        "\n",
        "prompt = \"The sky is\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "srbgggsgHqKu",
        "outputId": "2e595740-2ed4-4521-affe-e5fc7db0a7c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The sky is a vast expanse above us, often filled with clouds, stars, and the sun or moon, depending on the time of day. It can change colors with the time of day, displaying vibrant hues during sunrise and sunset. The sky also plays a crucial role in weather patterns and is home to various atmospheric phenomena. What specific aspect of the sky are you interested in?"
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "response.choices[0].message.content\n",
        "display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNWhB3VYHqKu"
      },
      "source": [
        "Try with different temperature to compare results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XZ9UdtDXHqKu"
      },
      "outputs": [],
      "source": [
        "params = set_open_params(temperature=0)\n",
        "response = get_completion(params, messages)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "H4We64XWHqKu",
        "outputId": "c91844cc-2cf3-4c7b-d17c-5b57e973fe7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The sky is a vast expanse above us, often filled with clouds, stars, and the sun or moon, depending on the time of day. It can change colors with the time of day, displaying vibrant hues during sunrise and sunset. The sky also plays a crucial role in weather patterns and is home to various atmospheric phenomena. What specific aspect of the sky are you interested in?"
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Print the response\n",
        "# print_in_markdown(response.choices[0].message.content)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdwuu3HSHqKv"
      },
      "source": [
        "### 1.1 Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CSG5a94pHqKv",
        "outputId": "1f41340f-8e5c-4a73-8870-1bbf093ab15e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Antibiotics are medications that combat bacterial infections by killing bacteria or inhibiting their reproduction, but they are ineffective against viruses and can contribute to antibiotic resistance if misused."
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "params = set_open_params(temperature=0.7)\n",
        "prompt = \"\"\"Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body's immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance.\n",
        "\n",
        "Explain the above in one sentence:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBX_fAzVHqKv"
      },
      "source": [
        "Exercise: Instruct the model to explain the paragraph in one sentence like \"I am 5\". Do you see any differences?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SULaJe5HqKv"
      },
      "source": [
        "### 1.2 Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5gFZ4l13HqKw",
        "outputId": "b88a7113-7c70-41f6-d9a6-bc24c96ebcd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Mice."
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.\n",
        "\n",
        "Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n",
        "\n",
        "Question: What was OKT3 originally sourced from?\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm5jC7pHHqKw"
      },
      "source": [
        "Context obtained from here: https://www.nature.com/articles/d41586-023-00400-x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v_UXbP2HqKw"
      },
      "source": [
        "Exercise: Edit prompt and get the model to respond that it isn't sure about the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4owoLk7HqKw"
      },
      "source": [
        "### 1.3 Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "EhBfD94wHqKw",
        "outputId": "f1b45ba9-c2b7-4778-c374-20be4c2d5489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Neutral"
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the text into neutral, negative or positive.\n",
        "\n",
        "Text: I think the food was okay.\n",
        "\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QJNkB5xHqKw"
      },
      "source": [
        "Exercise: Modify the prompt to instruct the model to provide an explanation to the answer selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaqrSd74HqKx"
      },
      "source": [
        "### 1.4 Role Playing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "roF8LCFOHqKx",
        "outputId": "afe97434-f4f5-4a63-9c24-0cf343e86960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Certainly! Black holes are formed through several astrophysical processes, primarily involving the gravitational collapse of massive objects. The most common mechanisms are:\n\n1. **Stellar Collapse**: When a massive star exhausts its nuclear fuel, it can no longer support itself against gravitational forces. The core collapses under its own gravity, and if the remaining mass is greater than the Tolman-Oppenheimer-Volkoff limit (approximately 2-3 solar masses), it continues to collapse, forming a black hole.\n\n2. **Supernova Events**: During the death of a massive star, a supernova explosion can occur. If the core remnant is sufficiently massive, it will collapse into a black hole post-explosion.\n\n3. **Merging Neutron Stars**: When two neutron stars in a binary system collide, the resulting mass can surpass the neutron star limit, leading to the formation of a black hole.\n\n4. **Primordial Black Holes**: These hypothetical black holes may have formed shortly after the Big Bang due to density fluctuations in the early universe. Their existence is still a topic of research.\n\n5. **Supermassive Black Holes**: Found at the centers of galaxies, these black holes can form through the merging of smaller black holes and the"
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n",
        "\n",
        "Human: Hello, who are you?\n",
        "AI: Greeting! I am an AI research assistant. How can I help you today?\n",
        "Human: Can you tell me about the creation of blackholes?\n",
        "AI:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JslYFSZdHqKx"
      },
      "source": [
        "Exercise: Modify the prompt to instruct the model to keep AI responses concise and short."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSANA8u1HqKx"
      },
      "source": [
        "### 1.5 Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "8XzI1xAeHqKx",
        "outputId": "1b81f8b3-c552-412c-ded4-6c15072862ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To retrieve all students from the Computer Science Department, you can use a SQL query that joins the `departments` table with the `students` table. Here is the SQL query:\n\n```sql\nSELECT s.StudentId, s.StudentName\nFROM students s\nJOIN departments d ON s.DepartmentId = d.DepartmentId\nWHERE d.DepartmentName = 'Computer Science';\n```\n\nThis query selects the `StudentId` and `StudentName` from the `students` table where the `DepartmentId` matches the `DepartmentId` in the `departments` table, filtering specifically for the department where the name is \"Computer Science\"."
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "prompt = \"\\\"\\\"\\\"\\nTable departments, columns = [DepartmentId, DepartmentName]\\nTable students, columns = [DepartmentId, StudentId, StudentName]\\nCreate a MySQL query for all students in the Computer Science Department\\n\\\"\\\"\\\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCebHJGYHqKx"
      },
      "source": [
        "### 1.6 Reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "pKNbJPHWHqKy",
        "outputId": "58fd6093-958f-4261-d2f6-8247fe8e785f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Let's break down the problem step by step.\n\n1. **Identify the odd numbers in the group:**\n   The group of numbers is: 15, 32, 5, 13, 82, 7, 1.\n   The odd numbers are: 15, 5, 13, 7, 1.\n\n2. **List the identified odd numbers:**\n   - 15\n   - 5\n   - 13\n   - 7\n   - 1\n\n3. **Add the identified odd numbers:**\n   Let's perform the addition step by step:\n   - First, add 15 + 5 = 20\n   - Next, add 20 + 13 = 33\n   - Then, add 33 + 7 = 40\n   - Finally, add 40 + 1 = 41\n\n   So, the sum of the odd numbers is:\n   \\[\n   15 + 5 + 13 + 7 + 1 = 41\n   \\]\n\n4. **Determine whether the result is odd or even:**\n   The sum, 41, is an odd number.\n\n5. **Conclusion:**\n   The odd numbers in the group add up"
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "\n",
        "Solve by breaking the problem into steps. First, identify the odd numbers, add them, and indicate whether the result is odd or even.\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqr6o8h7HqKy"
      },
      "source": [
        "Exercise: Improve the prompt to have a better structure and output format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7zvFo4xHqKy"
      },
      "source": [
        "## 2. Advanced Prompting Techniques\n",
        "\n",
        "Objectives:\n",
        "\n",
        "- Cover more advanced techniques for prompting: few-shot, chain-of-thoughts,..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dOG0rNxHqKy"
      },
      "source": [
        "### 2.2 Few-shot prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klHZtx3XHqKy",
        "outputId": "f4d0936e-1ac6-46f0-a75f-693ccf17c5b2"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "To determine if the odd numbers in the group add up to an even number, let's first identify the odd numbers in the set: 15, 5, 13, 7, and 1.\n",
              "\n",
              "Now, let's add them up:\n",
              "\n",
              "15 + 5 = 20  \n",
              "20 + 13 = 33  \n",
              "33 + 7 = 40  \n",
              "40 + 1 = 41  \n",
              "\n",
              "The sum of the odd numbers is 41, which is an odd number.\n",
              "\n",
              "Therefore, the answer is **False**."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
        "A: The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
        "A: The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
        "A: The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Di2XsfZHqKz"
      },
      "source": [
        "### 2.3 Chain-of-Thought (CoT) Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ilbS2viHqK4",
        "outputId": "60b060b3-0eac-44ac-dac1-255846db7924"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3vwTz0_HqK4"
      },
      "source": [
        "### 2.4 Zero-shot CoT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5CbvbdeHqK5",
        "outputId": "0b062c7d-2a9b-4714-966c-68046ad1a34b"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Step 1: Bought 10 apples.\n",
              "Step 2: Gave 2 apples to the neighbor and 2 apples to the repairman.\n",
              "Remaining apples: 10 - 2 - 2 = 6 apples.\n",
              "Step 3: Bought 5 more apples.\n",
              "Total apples now: 6 + 5 = 11 apples.\n",
              "Step 4: Ate 1 apple.\n",
              "Remaining apples: 11 - 1 = 10 apples.\n",
              "\n",
              "Final answer: You remained with 10 apples."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
        "\n",
        "Let's think step by step.\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(params, messages)\n",
        "IPython.display.Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbL_QNAqHqK5"
      },
      "source": [
        "### 2.5 Self-Consistency\n",
        "As an exercise, check examples in our [guide](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-advanced-usage.md#self-consistency) and try them here.\n",
        "\n",
        "### 2.6 Generate Knowledge Prompting\n",
        "\n",
        "As an exercise, check examples in our [guide](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-advanced-usage.md#generated-knowledge-prompting) and try them here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-VaAygIHqK5"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "example",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}